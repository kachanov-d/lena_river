{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "bublik",
      "language": "python",
      "name": "bublik"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "name": "data_preprocessing.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "welcome-identifier"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sqlalchemy as sq\n",
        "import scipy as sp"
      ],
      "id": "welcome-identifier",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naval-polls"
      },
      "source": [
        "class hydropost:\n",
        "    \"\"\"\n",
        "    Класс hydropost создан для начальной выгрузки, агрегации, предобработки данных с -гидро, -метео постов.\n",
        "    Класс является родительским для класса postmodel, и служит для полуавтоматической предобработки данных.\n",
        "    Важнейший атрибут класса- .hydroframe датафрейм, хранящий все данные, используемые для обучения моделей.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 loc_id='3010',              #id гидропоста, 4 цифры без первого нуля\n",
        "                 high_of_measurement=281):   #Высота над нулем поста. Необязательный параметр,\n",
        "                                             #используется для перехода от высоты уровня воды над нулем поста к абсолютной высоте над уровнем моря. Брал с allrivers\n",
        "\n",
        "        self.loc_id = loc_id\n",
        "\n",
        "        connectionstring ='dialect+driver://username:password@host:port/database' # Здесь необходимо вставить connectionstring пользователя по образцу\n",
        "        engine = sq.create_engine(connectionstring)\n",
        "        self.connection = engine.connect()\n",
        "        station_info_sql = 'SELECT station_id, station_name, ST_X(geom) AS lat, ST_Y(geom) AS lon, geom FROM atlas.hydro_stations_view WHERE station_id='+str(loc_id)\n",
        "        station_data_sql='''SELECT ts AS date,\n",
        "            min_level,\n",
        "            max_level,\n",
        "            avg_level,\n",
        "            water_temperature,\n",
        "            discharge, snow_depth,\n",
        "            ice_thickness,\n",
        "            precipitation_amount\n",
        "            FROM rosgidromet.hydro_arch\n",
        "            WHERE identifier='''+str(loc_id)\n",
        "\n",
        "        self.station_info = pd.read_sql(station_info_sql,self.connection)\n",
        "        station_data=pd.read_sql(station_data_sql,self.connection)\n",
        "        self.coordinats = {'lat': float(self.station_info['lat']),  #Храним координаты гидропоста для выбора ближайших метеопостов\n",
        "                           'lon': float(self.station_info['lon']),\n",
        "                           'geom':self.station_info['geom'].values[0] }\n",
        "        #Формируем рабочий датафрейм\n",
        "        self.hydroframe=station_data\n",
        "        self.hydroframe=self.hydroframe[~self.hydroframe['date'].duplicated(keep='last')]  #Избавляемся от дубликатов по дате.\n",
        "        self.hydroframe = self.hydroframe.set_index('date')\n",
        "        self.hydroframe.index=pd.to_datetime(self.hydroframe.index.date)\n",
        "        self.hydroframe.sort_index(inplace=True)\n",
        "        self.hydroframe=self.hydroframe.asfreq(freq='d', fill_value=None)\n",
        "        self.hydroframe['max_level'] += high_of_measurement         #Для удобства добавляем высоту над нулем поста к данным,\n",
        "        self.hydroframe['min_level'] += high_of_measurement         #тогда, текущая единица измерения- сантиметры над уровнем моря.\n",
        "        self.hydroframe['avg_level'] += high_of_measurement\n",
        "        self.hydroframe['minmaxdelta']=self.hydroframe['max_level']-self.hydroframe['min_level']        #Создаем переменную абсолютного изменения за сутки.\n",
        "        self.hydroframe.drop(columns=['min_level','avg_level'], inplace=True)                           #и удаляем неинформативные признаки\n",
        "        self.hydroframe[\"day_cos\"] = [np.cos(x * (2 * np.pi / 365)) for x in self.hydroframe.index.dayofyear]  #Добавляем время, разложенное в тригонометрическую функцию .\n",
        "        self.hydroframe[\"day_sin\"] = [np.sin(x * (2 * np.pi / 365)) for x in self.hydroframe.index.dayofyear]\n",
        "        self.hydroframe.dropna(how='all', inplace=True, axis=1)                                                #Удаляем пустые столбцы.\n",
        "\n",
        "\n",
        "    def add_snowdata(self,\n",
        "                     n_nearest=5,   # Количество ближайших к гидропосту метеопостов, данные с которых будем выбирать\n",
        "                     addlags=None,  # None или максимальный временной лаг выбираемых признаков.\n",
        "                     add_count=5):  # Количество признаков, добавляемых к датафрейму в результате выполнения.\n",
        "        \"\"\"\n",
        "        Метод добавляет к hydroframe количество признаков add_count из данных маршрутных снегосъемок n_nearest ближайших постов.\n",
        "        С одного метеопоста берется не более одного признака. В ходе выполнения данные выгружаются из озера, лаггируются на интервале\n",
        "        addlags, ранжируются в соответствии с модулем корреляции по убыванию, добавляются самые скоррелированные с max_level\n",
        "        \"\"\"\n",
        "\n",
        "        nearest_stations_sql='SELECT DISTINCT atlas.aisori_snow.station_id,'\\\n",
        "            +'rosgidromet.stations.location,' +'rosgidromet.stations.location <-> '+\"'\"+ self.coordinats['geom']+\"' as dist \" \\\n",
        "            +'''FROM atlas.aisori_snow, rosgidromet.stations\n",
        "            WHERE atlas.aisori_snow.station_id::int = rosgidromet.stations.identifier::int\n",
        "            ORDER BY dist''' +' '+ 'limit ' +str(n_nearest)\n",
        "\n",
        "        self.nearest_snowstations = pd.read_sql(nearest_stations_sql,self.connection) #Выгружаем список ближайших к гидропосту метеопосты\n",
        "\n",
        "        most_corr_per_post={}\n",
        "        for post in self.nearest_snowstations['station_id']:\n",
        "\n",
        "            #С метеопоста выгружаем типы признаков, показавших наибольшую корреляцию c max_level в ходе предварительных экспериментов\n",
        "            snowstation_data=pd.read_sql('''SELECT\n",
        "                                dt as date,\n",
        "                                route_type,\n",
        "                                station_snow_coverage,\n",
        "                                route_snow_coverage,\n",
        "                                snow_cover_average_height,\n",
        "                                snow_cover_max_height,\n",
        "                                snow_density,\n",
        "                                nature_of_snow_cover_occurrence,\n",
        "                                snow_cover_nature\n",
        "                                FROM atlas.aisori_snow\n",
        "                                WHERE station_id =''' \\\n",
        "                                +str(post) +\"and dt>=TO_DATE('\" +str(self.hydroframe.index[0])+\"','YYYY-MM-DD')\" +\" and dt<=TO_DATE('\"+ str(self.hydroframe.index[-1])+\"','YYYY-MM-DD')\"+ 'ORDER BY date'   ,connection)\n",
        "\n",
        "            workframe_all_routes=pd.DataFrame(self.hydroframe['max_level'])\n",
        "\n",
        "            for route in range(1,4):     \n",
        "                                                                                           #На метеопосте осуществляется проходка по одному или нескольким из трех возможных маршрутов\n",
        "                workframe=snowstation_data.loc[snowstation_data['route_type']==route]\\\n",
        "                .set_index('date').drop(columns=['route_type'])\\\n",
        "                    .add_prefix(str(post)+'_r_'+str(route)+'_')\n",
        "                workframe.index=pd.to_datetime(workframe.index)\n",
        "                workframe.index=workframe.index.date\n",
        "                workframe.sort_index(inplace=True)\n",
        "                workframe_all_routes=pd.concat([workframe_all_routes,workframe],axis=1)\n",
        "\n",
        "            \n",
        "            workframe_all_routes.dropna(how='all', axis='columns', inplace=True)                                                      #Получили workframe_all_routes датафрейм, содержащий признаки на каждом из доступных маршрутов\n",
        "            workframe_all_routes=workframe_all_routes.interpolate(method='linear', limit_area='inside', limit=30)                     #Интерполируем для заполнения пробелов между измерениями\n",
        "            workframe_all_routes=workframe_all_routes.loc[:,(workframe_all_routes.isna().sum()/workframe_all_routes.shape[0] < 0.5)]  #Удаляем признаки с пропускми больше 50%. Для более теплых районов следует подумать над коэффициентом\n",
        "            workframe_all_routes=workframe_all_routes.fillna(0)\n",
        "\n",
        "            if addlags!=None:   #Лаггируем признаки для добавления в фрейм\n",
        "                lagged_corr=pd.DataFrame()\n",
        "                for lag in range(1, addlags+1):\n",
        "                    lagged_corr=pd.concat([lagged_corr,\n",
        "                                           workframe_all_routes.drop(columns=['max_level']).shift(lag).iloc[(lag+1):].add_suffix(('_lag_'+str(lag))) ], axis=1 )\n",
        "\n",
        "                workframe_all_routes=pd.concat([workframe_all_routes, lagged_corr], axis=1)\n",
        "                                                                                                                                      #Ранжируем признаки в соответстваии с коэффицентом корреляции, берем add_count признаков\n",
        "                corr_rank=pd.DataFrame(workframe_all_routes.drop(columns=['max_level'])\\\n",
        "                                       .corrwith(workframe_all_routes['max_level'])\\\n",
        "                                       .abs().sort_values(ascending=False).head(add_count))\n",
        "\n",
        "            try:    #добавляем в словарь ключ- модуль коэффициета корреляции, значение- самый скоррелированный признак\n",
        "                most_corr_per_post[float(corr_rank.iloc[0])]= workframe_all_routes[corr_rank.index[0]]\n",
        "            except IndexError:\n",
        "                continue\n",
        "        #Из отсортированного по ключу (модуль коэффициентакорреляции) most_corr_per_post словаря добавляем add_count признаков к hydroframe\n",
        "        for key in sorted(most_corr_per_post.keys(), reverse=True)[:add_count]:\n",
        "            most_corr_per_post[key].index=pd.to_datetime(most_corr_per_post[key].index)\n",
        "            self.hydroframe=self.hydroframe.merge(most_corr_per_post[key].fillna(0), right_index=True, left_index=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def add_weather_data(self, n_nearest=5): # n_nearest- из скольки ближайших постов формировать признаки\n",
        "        \"\"\"\n",
        "        Метод добавляет к hydroframe климатические признаки. Значения максимальной, минимальной температуры воздуха, скорости ветра, влажности\n",
        "        усредняются, значения дневных осадков усредняются с весовыми коэффициентами до достижения максимальной корреляции с max_level\n",
        "        \"\"\"\n",
        "\n",
        "        nearest_stations_sql='''select distinct on (dist)\n",
        "                                stationid, wkb_geometry, namelong,\n",
        "                                wkb_geometry <->''' +\"'\"+ self.coordinats['geom']+\"' as dist \"+\\\n",
        "                                '''from (select * from (select\n",
        "                                distinct identifier from rosgidromet.meteo_daily) as tab\n",
        "                                inner join rosgidromet.asunp_station on rosgidromet.asunp_station.stationid= tab.identifier\n",
        "                                order by wkb_geometry <->'''+\\\n",
        "                                \"'\"+ self.coordinats['geom']+\"'\"+\\\n",
        "                                'limit 1000) as tab2 where meteo::float != 0 order by dist limit '+ str(n_nearest)\n",
        "\n",
        "        self.nearest_meteostations = pd.read_sql(nearest_stations_sql, self.connection)\n",
        "        meteostation_data={}\n",
        "        for station in self.nearest_meteostations['stationid']:\n",
        "\n",
        "            meteostation_data[station]=pd.read_sql('''select dt as date,\n",
        "                                        maxairtemperature,\n",
        "                                        minairtemperature,\n",
        "                                        soiltemperature,\n",
        "                                        totalaccumulatedprecipitation,\n",
        "                                        windspeed,\n",
        "                                        maxrelativehumidity\n",
        "                            from rosgidromet.meteo_daily\n",
        "                            where identifier::float =''' +str(station)\\\n",
        "                            +\" and dt>=TO_DATE('\" + str(self.hydroframe.index[0])\\\n",
        "                            + \" ','YYYY-MM-DD') and dt<=TO_DATE('\" +str(self.hydroframe.index[-1]) + \" ' ,'YYYY-MM-DD') ORDER BY date\" , connection)\\\n",
        "                            .set_index('date')\n",
        "            meteostation_data[station]=meteostation_data[station][~meteostation_data[station].index.duplicated(keep='first')]\n",
        "\n",
        "        for feature in ['maxairtemperature',\n",
        "                        'minairtemperature',    #   'soiltemperature', - пропущенны данные по всем постам в 20,19, 21 годах\n",
        "                        'windspeed',\n",
        "                        'maxrelativehumidity']:\n",
        "            for station in meteostation_data:\n",
        "                frame=pd.concat([meteostation_data[station][feature] for station in meteostation_data], axis=1)\n",
        "                frame=frame.loc[:,(frame.isna().sum()/frame.shape[0] < 0.5)]\n",
        "                frame=frame.mean(axis=1) \n",
        "                frame.name=feature+'_mean'\n",
        "            self.hydroframe=self.hydroframe.merge(frame, right_index=True, left_index=True,how='right')\n",
        "\n",
        "        self.weights={}\n",
        "        for feature in ['totalaccumulatedprecipitation']:\n",
        "            for station in meteostation_data:\n",
        "                precipitation_frame=pd.concat([meteostation_data[station][feature].rename(station+'_'+feature, inplace=True) for station in meteostation_data], axis=1)\n",
        "                precipitation_frame.dropna(how='all', axis='columns', inplace=True)\n",
        "                precipitation_frame.fillna(0, inplace=True)\n",
        "\n",
        "            bnds=[(0.0, 1.0)]*precipitation_frame.shape[1]\n",
        "            constraints=[{'type':'eq', 'fun':lambda x: 1-sum(x)}]\n",
        "\n",
        "            def function(weights=[],ret_frame=False):\n",
        "                frame=precipitation_frame.copy()\n",
        "                for num, col in enumerate(frame):\n",
        "                    frame[col]=frame[col]*weights[num]\n",
        "                frame=frame.sum(axis=1)\n",
        "                if ret_frame==False:\n",
        "                    return -abs(frame.corr(self.hydroframe['max_level']))\n",
        "                elif ret_frame==True:\n",
        "                    return frame\n",
        "\n",
        "            sol=sp.optimize.basinhopping(function,\n",
        "                                            [1/precipitation_frame.shape[1]]*precipitation_frame.shape[1],\n",
        "                                            niter=5,\n",
        "                                            stepsize=0.5,\n",
        "                                            T=0.01,\n",
        "                                            interval=5,\n",
        "                                            minimizer_kwargs={'bounds':bnds, 'constraints':constraints})\n",
        "\n",
        "            self.weights[feature]=dict(zip(precipitation_frame.columns, sol.x))\n",
        "            ret=function(sol.x, ret_frame=True).rename('weight_mean_'+feature)\n",
        "            ret.index=pd.to_datetime(ret.index)\n",
        "            self.hydroframe=self.hydroframe.merge(ret, right_index=True, left_index=True, how='right')\n",
        "\n",
        "\n",
        "\n",
        "    def diff_timeseries(self,\n",
        "                        features=['max_level',   # Список признаков, подлежащих дифференцированию\n",
        "                                  'water_temperature',\n",
        "                                  'air_temperature'],\n",
        "                        add=False,               # Добавить дифференцированный признак в датафрейм, или заменить исходный\n",
        "                        ret=False ):             # Вернуть признак вместо добавления или замены в hydroframe\n",
        "        \"\"\"Метод позволяет дифференцировать временные ряды hydroframe, хранить первое значение, восстанавливать \n",
        "        ранее дифференцированные временные ряды.\n",
        "        \"\"\"\n",
        "        if hasattr(self, 'firstval')==False:\n",
        "            self.firstval={}\n",
        "        if ret==False:\n",
        "            for feature in features:\n",
        "                self.firstval[feature]=self.hydroframe[feature].iloc[0]\n",
        "                if add==False:\n",
        "                    self.hydroframe[feature]=self.hydroframe[feature].diff()\n",
        "                elif add==True:\n",
        "                    self.hydroframe=self.hydroframe.merge(self.hydroframe[feature].diff().rename('diff_'+feature), left_index=True, right_index=True)\n",
        "        elif ret==True:\n",
        "            for feature in features:\n",
        "                self.hydroframe[feature]= np.r_[self.firstval[feature], self.hydroframe[feature].iloc[1:]].cumsum().astype('float64')\n",
        "\n",
        "\n",
        "\n",
        "    def interp_gaps(self, \n",
        "                    fillwith=None,    #Чем запронять значения, не попавшие в интервал интерполяции\n",
        "                    features=[],      #Признаки, подлежащие интерполяции \n",
        "                    between=10,       #Максимальное число пропусков между двумя значениями, подлежащие интерполяции\n",
        "                    method='linear'): #Метод интерполяции\n",
        "        \"\"\"\n",
        "        Интерполирование пропусков временных рядов features из hydroframe с условием заполнения не более чем between пропусков между двумя значениями,\n",
        "        и заполнением не интерполированных пропусков константами \n",
        "        \"\"\"\n",
        "        for feature in features:\n",
        "            self.hydroframe[feature]=self.hydroframe[feature].interpolate(method=method, limit_area='inside', limit=between)\n",
        "            if fillwith !=None:\n",
        "                self.hydroframe[feature]=self.hydroframe[feature].fillna(fillwith)\n",
        "\n",
        "\n",
        "\n",
        "    def log_timeseries(self, \n",
        "                       features=['max_level',\n",
        "                                 'min_level',\n",
        "                                 'avg_level',\n",
        "                                 'discharge'], \n",
        "                       inv=False):\n",
        "        \"\"\"\n",
        "        Логарифмирование временных рядов features из hydroframe с возможностью восстановления (экспоненциирования).\n",
        "        \"\"\"\n",
        "        for feature in features:\n",
        "            if inv==False:\n",
        "                self.hydroframe[feature]=np.log(self.hydroframe[feature])\n",
        "            else:\n",
        "                self.hydroframe[feature]=np.exp(self.hydroframe[feature])\n",
        "\n",
        "\n",
        "\n",
        "    def categorical_dummy(self,features=['water_code_status']):\n",
        "        \"\"\" \n",
        "        Даммификация кодов состояний водных объектов\n",
        "        \"\"\"\n",
        "        for feature in features:\n",
        "            self.hydroframe=self.hydroframe.merge(self.hydroframe[features].squeeze().str.get_dummies(sep=',').add_prefix('ksvo_'), left_index=True, right_index=True)\n",
        "            del(self.hydroframe[feature])\n",
        "\n",
        "\n",
        "            \n",
        "    def mean_timeseries(self,\n",
        "                        inverse=None,                  # Series, для которого нужно вернуть обратно преобразованный Series\n",
        "                        target='max_level',            # Имя переменной для обратного преобразования\n",
        "                        features=['avg_level',         # Признаки из hydroframe для прямого преобразования\n",
        "                                  'min_level',\n",
        "                                  'max_level',\n",
        "                                  'water_temperature',\n",
        "                                  'ice_thickness',\n",
        "                                  'snow_depth',\n",
        "                                  'air_temperature', \n",
        "                                  'precipitation_amount',\n",
        "                                  'discharge'],\n",
        "                        scaling=True,                   # Масштабирование признака на единичном интерваое\n",
        "                        ret=False):                     # Возвращать прямо преобразрванные признаки вместо изменения в фрейме\n",
        "        \"\"\"\n",
        "        Z-преобразование временных рядов из features с возможностью обратного преобразования.\n",
        "        Если scaling = True, данные просто нормализуются на интервале [0:1].\n",
        "        \"\"\"\n",
        "        if inverse is None:\n",
        "            self.frame_mean = self.hydroframe[features].mean()\n",
        "            self.frame_std= self.hydroframe[features].std()\n",
        "            norm_df = (self.hydroframe[features] - self.frame_mean) / self.frame_std\n",
        "        if (scaling==True) and (inverse is None):\n",
        "            self.max_val= norm_df.max()\n",
        "            self.min_val=norm_df.min()\n",
        "            norm_df=(norm_df-self.min_val)/(self.max_val-self.min_val)\n",
        "        if (ret==False) and (inverse is None):\n",
        "            self.hydroframe[features]=norm_df\n",
        "        elif ret==True:\n",
        "            return norm_df\n",
        "\n",
        "        if inverse is not None:\n",
        "            return (inverse*(self.max_val[target]-self.min_val[target])+self.min_val[target])*self.frame_std[target]+ self.frame_mean[target]\n",
        "\n",
        "\n",
        "\n",
        "    def cut_level_nas(self, will_nas=8):\n",
        "        \"\"\"\n",
        "        Вырезать участки фрейма, для которых max_level is NaN,  пометить семь следующих дней и will_nas предыдущих категориальными признаками\n",
        "        \"\"\"\n",
        "        self.hydroframe['was_na']=(self.hydroframe['max_level'].isna().replace({False: 0, True:1}).shift(7)).fillna(0)\n",
        "        self.hydroframe['will_na']=(self.hydroframe['max_level'].isna().replace({False: 0, True:1}).shift(-will_nas)).fillna(0)\n",
        "        self.hydroframe=self.hydroframe.loc[~self.hydroframe['max_level'].isna()]\n",
        "\n",
        "\n",
        "    def something_like_mrw(self, \n",
        "                           features=[],   # Признаки для которых применить преобразование.\n",
        "                           window=30,     # Размер окна.\n",
        "                           inv=None,      # Series, обратно преобразованный который нужно вернуть.\n",
        "                           target=None):  # Имя обратно преобразовываемого Series.\n",
        "        \"\"\"\n",
        "        Метод делает что-то вроде Moving Average Smoothing для случая, когда сезонные пики не предопределены по дням в году.\n",
        "        Подробнее описано в отчете о НИР. \n",
        "        \"\"\"\n",
        "        if inv is None:\n",
        "            if hasattr(self, 'rolling_mean_by_day')==False:\n",
        "                self.rolling_mean_by_day={}\n",
        "            for feature in features:\n",
        "                self.rolling_mean_by_day[feature]=self.hydroframe[feature].rolling(window, center=True).mean().groupby(self.hydroframe.index.dayofyear).median()\n",
        "                self.hydroframe[feature]-=pd.Series([i for i in self.hydroframe.index.dayofyear], index=self.hydroframe.index ).apply(lambda x: self.rolling_mean_by_day[feature][x])\n",
        "\n",
        "        elif inv is not None:\n",
        "            return inv+pd.Series([i for i in inv.index.dayofyear], index=inv.index ).apply(lambda x: self.rolling_mean_by_day[target][x])\n"
      ],
      "id": "naval-polls",
      "execution_count": null,
      "outputs": []
    }
  ]
}